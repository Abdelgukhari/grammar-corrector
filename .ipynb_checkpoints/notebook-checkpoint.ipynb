{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required packages\n",
    "%pip install transformers symspellpy\n",
    "\n",
    "from transformers import T5ForConditionalGeneration, T5Tokenizer\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load Pre-trained Grammar Correction Model\n",
    "model_name = \"vennify/t5-base-grammar-correction\"\n",
    "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "model = T5ForConditionalGeneration.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load SymSpell for Spelling Correction\n",
    "symspell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\n",
    "    \"symspellpy\", \"frequency_dictionary_en_82_765.txt\"\n",
    ")\n",
    "symspell.load_dictionary(dictionary_path, term_index=0, count_index=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_spelling(text):\n",
    "    \"\"\"Fix spelling errors.\"\"\"\n",
    "    words = text.split()\n",
    "    corrected_words = []\n",
    "    for word in words:\n",
    "        suggestions = symspell.lookup(word, Verbosity.CLOSEST, max_edit_distance=2)\n",
    "        corrected_word = suggestions[0].term if suggestions else word\n",
    "        corrected_words.append(corrected_word)\n",
    "    return \" \".join(corrected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_grammar(text):\n",
    "    \"\"\"Fix grammar errors.\"\"\"\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")\n",
    "    outputs = model.generate(**inputs, max_length=len(text.split()) + 10)\n",
    "    return tokenizer.decode(outputs[0], skip_special_tokens=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive Testing\n",
    "input_text = \"She go to the park evryday.\"\n",
    "print(f\"Original: {input_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix spelling first\n",
    "spelling_fixed = correct_spelling(input_text)\n",
    "print(f\"Spelling Fixed: {spelling_fixed}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix grammar next\n",
    "grammar_fixed = correct_grammar(spelling_fixed)\n",
    "print(f\"Final Correction: {grammar_fixed}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
